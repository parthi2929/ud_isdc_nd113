# Part 2

We will now see, what happens to probability distributions when robot moves.

<strong>Situation</strong>: We have a 1 D world space with 5 grid cells. Robot could be anywhere across this space and we assume a probability distribution like below (say, due to a previous sense function, few cells have more probability). That is, we assume robot has equally likely chance of being either at X<sub>2</sub> or X<sub>4</sub>, and not anywhere else. Also world is cyclic that is after X<sub>5</sub> , next cell is X<sub>1</sub> .


<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_08h57_07.png"/>
<figcaption align='center'>Figure 1</figcaption>
</figure>
</div>

If X<sub>i</sub> represents the cell location, then

$$
 \displaystyle \begin{array}{l}p({{X}_{1}})=0\\p({{X}_{2}})=0.5\\p({{X}_{3}})=0\\p({{X}_{4}})=0.5\\p({{X}_{5}})=0\end{array} \tag{1}
$$

Let us also assume robot moves in 2 steps, always to its right side, and it's motion accuracy as below.

<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_08h58_25.png"/>
<figcaption align='center'>Figure 2</figcaption>
</figure>
</div>

<strong>Problem</strong>: Robot moves 2 steps right. What is the probability of robot location in each cell X<sub>i</sub> ? Appears like posterior, but we will get to that soon.

Intuitively, since the robot as per our assumption could have moved only from X<span style="font-size:13.3333px;">2</span> or X<span style="font-size:13.3333px;">4</span> , their probability should decrease, and probable target cells' probability increase. Generically, any cell's probability, depends on robot arriving at it from adjacent cells' either by undershoot or exact or overshoot from cells' left side (caz robot always move in right direction). For eg, For X<sub>1</sub> ,the only way robot would have arrived is if robot was earlier in X<sub>4</sub> and did exact 2 steps motion to its right.

Since both probability (prior and posterior) is on same sample space "location" we shall differentiate the event by time notation for easier understanding. Further, there is also mix up of probabilities (one cell might receive robot from more than one source. For eg, X<sub>5</sub> might have received either by robot undershooting from X<sub>4</sub> or overshooting from X<sub>2</sub> .

We will thus divide the cases by all possible current locations, calculate the probabilities individually and then add up. Don't worry, it is for this purpose, we assumed only 2 locations having prior probability of having the robot. We have thus only 2 cases as below.

<strong>Case A</strong>: Robot moving from X<sub>2</sub> .  Including time notation, we could visualize its further movement with probabilities as below.

For example, out of 50% of being at X<sub>2</sub>, robot has 10% chance of ending up at X<sub>3</sub> after its movement, as it might undershoot. Other locations could be understood in similar manner.


<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_10h24_25.png"/>
<figcaption align='center'>Figure 3: Case A</figcaption>
</figure>
</div>

<strong>Case B</strong>: Robot moving from X<sub>4</sub>. Its visualization could be as follows.


<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_10h31_34.png"/>
<figcaption align='center'>Figure 4: Case B</figcaption>
</figure>
</div>

Now, for each cell's probability, we could calculate simply $ \displaystyle p(A\cup B)$ as below.

$$ 
\displaystyle p(A\cup B)=p(A)+p(B)-p(A\cap B)=p(A)+p(B) \tag{2}
$$

$ \displaystyle p(A\cap B)=0$  because, robot could have moved either from X<sub>2</sub> or X<sub>4</sub> not both. (We are not including quantum effects here ;).

Now we can derive individual cell's probabilities in both cases as below. For eg, in case A, $ \displaystyle p(X_{1}^{t})=0$ because, there is no way, robot could have arrived at X<sub>1</sub> from X<sub>2</sub> .  Similarly for others. Make sure you fully able to  comprehend below equations , as we would generalize this shortly.

$ \displaystyle p(A)$:

$$ \displaystyle \begin{array}{l}p(X_{1}^{t})=0\\p(X_{2}^{t})=0\\p(X_{3}^{t})=p(X_{2}^{{t-1}})p(X_{3}^{t}|X_{2}^{{t-1}})=p(X_{2}^{{t-1}})(0.1)=(0.5)(0.1)=0.05\\p(X_{4}^{t})=p(X_{2}^{{t-1}})p(X_{4}^{t}|X_{2}^{{t-1}})=p(X_{2}^{{t-1}})(0.8)=(0.5)(0.8)=0.40\\p(X_{5}^{t})=p(X_{2}^{{t-1}})p(X_{5}^{t}|X_{2}^{{t-1}})=p(X_{2}^{{t-1}})(0.1)=(0.5)(0.1)=0.05\end{array} \tag{3}
$$

$ \displaystyle p(B)$:

$$ \displaystyle \begin{array}{l}p(X_{1}^{t})=p(X_{4}^{{t-1}})p(X_{1}^{t}|X_{4}^{{t-1}})=p(X_{4}^{{t-1}})(0.8)=(0.5)(0.8)=0.40\\p(X_{2}^{t})=p(X_{4}^{{t-1}})p(X_{2}^{t}|X_{4}^{{t-1}})=p(X_{4}^{{t-1}})(0.1)=(0.5)(0.1)=0.05\\p(X_{3}^{t})=0\\p(X_{4}^{t})=0\\p(X_{5}^{t})=p(X_{4}^{{t-1}})p(X_{5}^{t}|X_{4}^{{t-1}})=p(X_{4}^{{t-1}})(0.1)=(0.5)(0.1)=0.05\end{array} \tag{4} $$

Combining (3)and (4) using (2), we thus could arrive at final probabilities of each cell.

$$ \displaystyle \begin{array}{l}p(X_{1}^{t})=p(X_{4}^{{t-1}})p(X_{1}^{t}|X_{4}^{{t-1}})=p(X_{4}^{{t-1}})(0.8)=(0.5)(0.8)=0.40\\p(X_{2}^{t})=p(X_{4}^{{t-1}})p(X_{2}^{t}|X_{4}^{{t-1}})=p(X_{4}^{{t-1}})(0.1)=(0.5)(0.1)=0.05\\p(X_{3}^{t})=p(X_{2}^{{t-1}})p(X_{3}^{t}|X_{2}^{{t-1}})=p(X_{2}^{{t-1}})(0.1)=(0.5)(0.1)=0.05\\p(X_{4}^{t})=p(X_{2}^{{t-1}})p(X_{4}^{t}|X_{2}^{{t-1}})=p(X_{2}^{{t-1}})(0.8)=(0.5)(0.8)=0.40\\\\p(X_{5}^{t})=p(X_{2}^{{t-1}})p(X_{5}^{t}|X_{2}^{{t-1}})+p(X_{4}^{{t-1}})p(X_{5}^{t}|X_{4}^{{t-1}})\\=p(X_{2}^{{t-1}})(0.1)+p(X_{4}^{{t-1}})(0.1)=(0.5)(0.1)+(0.5)(0.1)=0.1\end{array} \tag{5}$$

Voila. We derived final probabilities. Visualizing,


<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_11h01_52.png"/>
<figcaption align='center'>Figure 5</figcaption>
</figure>
</div>

Note that, all $ \displaystyle {p(X_{i}^{t})}$ add up to 1. That is,

$$ \displaystyle \sum\limits_{{k=1}}^{n}{{p(X_{i}^{t})=1}}$$

Thus, $ \displaystyle {p(X_{i}^{t})}$ is <span style="color:#0000ff;">already a posterior probability or same as joint probability as no distinguishing between them needed</span>.  This is an important inference. Further,<span style="color:#0000ff;"> "movement" only caused the distribution to "shift" in direction of robot</span> to other cells, unlike our earlier case "<a href="https://parthibanrajendran.wordpress.com/2018/03/21/5-robot-localization-sense/" target="_blank" rel="noopener">sense</a>".

<strong>Food for thought:</strong> Try a simpler case as below for move example, with 100% certainty for initial location and exact movement of robot with no under or overshoot and these would be much more evident.


<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_13h44_31.png"/>
<figcaption align='center'>Figure 6</figcaption>
</figure>
</div>

<strong>Generalization</strong>:

Coming back to our initial example,  lets examine say X<sub>1</sub> . We know, robot could have come to it only from X<sub>4</sub> in our case, so possibility from all other cells would be 0.

$ \displaystyle p(X_{1}^{t})=0+p(X_{4}^{{t-1}})p(X_{1}^{t}|X_{4}^{{t-1}})$

If we try to generalize case of X<sub>1</sub> to include all locations, it would be like below and only that except $ \displaystyle p(X_{4}^{{t-1}})p(X_{1}^{t}|X_{4}^{{t-1}})$, other factors ended up 0 as below. For eg, $ \displaystyle p(X_{1}^{t}|X_{2}^{{t-1}})$ is 0 because, no way robot would have come from X<sub>2</sub> to X<sub>1</sub>.

$$ \displaystyle \begin{array}{l}p(X_{1}^{t})=p(X_{1}^{{t-1}})p(X_{1}^{t}|X_{1}^{{t-1}})+p(X_{2}^{{t-1}})p(X_{1}^{t}|X_{2}^{{t-1}})\\+p(X_{3}^{{t-1}})p(X_{1}^{t}|X_{3}^{{t-1}})+p(X_{4}^{{t-1}})p(X_{1}^{t}|X_{4}^{{t-1}})+p(X_{5}^{{t-1}})p(X_{1}^{t}|X_{5}^{{t-1}})\\\\=p(X_{1}^{{t-1}})(0)+p(X_{2}^{{t-1}})(0)+p(X_{3}^{{t-1}})(0)+p(X_{4}^{{t-1}})(0.1)+p(X_{5}^{{t-1}})(0)\\=p(X_{1}^{{t-1}})(0)+p(X_{2}^{{t-1}})(0)+p(X_{3}^{{t-1}})(0)+(0.5)(0.8)+p(X_{5}^{{t-1}})(0)=0.40\end{array} \tag{6} $$

Generalizing this further for any cell X<sub>i</sub>, we get,

$$ \displaystyle \begin{array}{l}p(X_{i}^{t})=p(X_{1}^{{t-1}})p(X_{i}^{t}|X_{1}^{{t-1}})+p(X_{2}^{{t-1}})p(X_{i}^{t}|X_{2}^{{t-1}})\\+p(X_{3}^{{t-1}})p(X_{i}^{t}|X_{3}^{{t-1}})+p(X_{4}^{{t-1}})p(X_{i}^{t}|X_{4}^{{t-1}})+p(X_{5}^{{t-1}})p(X_{i}^{t}|X_{5}^{{t-1}})\end{array} \tag{7} $$

Formulating, we get:

$$ \displaystyle p(X_{i}^{t})=\sum\limits_{{k=1}}^{n}{{p(X_{k}^{{t-1}})p(X_{i}^{t}|X_{k}^{{t-1}})}} \tag{8} $$

This equation above is called "<span style="color:#0000ff;">Theorem of Total Probability</span>". The above equation is responsible for shifting of the distribution and this  mechanism is generally called "<span style="color:#0000ff;">convolution</span>". Sounds cool, eh?!

<strong>Python Implementation:</strong>

Assume,

<span style="color:#0000ff;"><em>p</em></span> = list of prior probabilities of all cells
<span style="color:#0000ff;"><i>U</i></span> = robot's movement in steps (in our case, its 2)
<span style="color:#0000ff;"><em>pExact</em> </span>= probability of robot exactly finishes 2 steps and ends up at <strong>U</strong>th (2nd in our case) cell to its right
<span style="color:#0000ff;"><i>pUndershoot </i></span>= probability of robot undershoots, and ends up at <strong>(U-1)th</strong> cell (1st cell in our case) to its right
<span style="color:#0000ff;"><em>pOvershoot</em></span> = probability of robot overshoots and ends up at <strong>(U+1)th</strong> cell (3rd cell in our case) to its right

The approach is little bit tricky. So let's hard code for our logic and see if we could derive general code out of it. Let us take another list '<strong>q</strong>' same size as p and try to calculate c<strong>ase A</strong>. We could visualize as below.



<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_11h43_02.png"/>
<figcaption align='center'>Figure 7: Case A</figcaption>
</figure>
</div>

Then we could readily calculate, non zero values of <strong>q</strong> from diagram as below.

<p style="text-align:left;">q[2] = p[1](0.1) = (0.5)(0.1) = 0.05
q[3] = p[1](0.8) = (0.5)(0.8) = 0.40
q[4] = p[1](0.1) = (0.5)(0.1) = 0.05<span style="float:right;">(9)</span></p>

Now the <strong>'new' q</strong> would be as below.

<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_12h02_19.png"/>
<figcaption align='center'>Figure 8: Case B on top of Case A</figcaption>
</figure>
</div>

Now <strong>case B</strong>, but on <strong>'new' q</strong>, because we should not lose old q values as probabilities add up. Recall (2).

<p style="text-align:left;">q[0] = p[3](0.8) + q[0] = (0.5)(0.8) + 0 = 0.40
q[1] = p[3](0.1) + q[1] = (0.5)(0.1) + 0 = 0.05
q[4] = p[3](0.1) + q[4] = (0.5)(0.1) + 0.05 = 0.10<span style="float:right;">(10)</span></p>

Note how case A and B added up for q[4]. The resultant distribution might look like this:

<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_12h25_20.png"/>
<figcaption align='center'>Figure 9: Final distribution in list 'q'</figcaption>
</figure>
</div>

Generalizing from both above equations, and re writing again we get:

q[2] = p<a href="0.1">1</a> + q[2] = (0.5)(0.1) + 0 = 0.05 <span style="float:right;">#undershoot</span>
q[3] = p<a href="0.8">1</a> + q[3] = (0.5)(0.8) + 0 = 0.40 <span style="float:right;">#exact</span>
q[4] = p<a href="0.1">1</a> + q[4] = (0.5)(0.1) + 0 = 0.05 <span style="float:right;">#overshoot</span>

q[4] = p<a href="0.1">3</a> + q[4] = (0.5)(0.1) + 0.05 = 0.10 <span style="float:right;">#undershoot</span>
q[0] = p<a href="0.8">3</a> + q[0] = (0.5)(0.8) + 0 = 0.40 <span style="float:right;">#exact</span>
q[1] = p<a href="0.1">3</a> + q[1] = (0.5)(0.1) + 0 = 0.05 <span style="float:right;">#overshoot</span>

Note that, we calculated only for 2 cases involving <strong>p[1]</strong> and <strong>p[3]</strong>, as it was evident from our case. We could extend this for all p elements. Visualize to imagine target q elements for each element of p to calculate its probabilities.

q[1] = p<a href="0.1">0</a> + q[1]
q[2] = p<a href="0.8">0</a> + q[2]
q[3] = p<a href="0.1">0</a> + q[3]

q[2] = p<a href="0.1">1</a> + q[2]
q[3] = p<a href="0.8">1</a> + q[3]
q[4] = p<a href="0.1">1</a> + q[4]

q[3] = p<a href="0.1">2</a> + q[3]
q[4] = p<a href="0.8">2</a> + q[4]
q[0] = p<a href="0.1">2</a> + q[0]

q[4] = p<a href="0.1">3</a> + q[4]
q[0] = p<a href="0.8">3</a> + q[0]
q[1] = p<a href="0.1">3</a> + q[1]

q[0] = p<a href="0.1">4</a> + q[0]
q[1] = p<a href="0.8">4</a> + q[1]
q[2] = p<a href="0.1">4</a> + q[2]

Some might have already observed an emerging pattern. Let us quickly check the code, if this works fine before we generalize.

In below code, we just create an empty q list of same size as p, (and a temp list for undershoot/exact/overshoot probabilities). Then we update for each element p with above logic and then simply return the updated q.

```python
p=[0, 0.5, 0, 0.5, 0]
pExact = 0.8
pOvershoot = 0.1
pUndershoot = 0.1
def move(p, U):
    """
    p = given probability distribution list
    U = amount of steps robot has moved to right (assumed as right for now)
    """
    # INEXACT MOTION
    q = [ 0 for _ in p ]
    temp = [pUndershoot, pExact, pOvershoot]
    q[1] = p[0]<em>temp[0] + q[1]
    q[2] = p[0]</em>temp[1] + q[2]
    q[3] = p[0]*temp[2] + q[3]

<pre><code>q[2] = p[1]*temp[0] + q[2]
q[3] = p[1]*temp[1] + q[3]
q[4] = p[1]*temp[2] + q[4]

q[3] = p[2]*temp[0] + q[3]
q[4] = p[2]*temp[1] + q[4]
q[0] = p[2]*temp[2] + q[0]

q[4] = p[3]*temp[0] + q[4]
q[0] = p[3]*temp[1] + q[0]
q[1] = p[3]*temp[2] + q[1]

q[0] = p[4]*temp[0] + q[0]
q[1] = p[4]*temp[1] + q[1]
q[2] = p[4]*temp[2] + q[2]
return q


print("Prior probability: " + str(p))
print("Posterior probability: " + str(move(p,2)))
```

Output:
<img class="alignnone size-full wp-image-190" src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_12h47_36.png" alt="2018-03-22_12h47_36" width="386" height="57" />


Great, this works. Now let us condense it to generalize. We need to run for all elements of p, so that calls for a for loop. Tricky part is the indexing of q. Check the flow of q indexes. Its { (1,2,3) , (2,3,4), (3,4,0) ...}

If we run a for loop, the p's index could be used to calculate q's index by using undershoot, exact and overshoot target cell location relative to current p's index.

For example, in case of <strong>p[1]</strong>, we could also re write as below (remember, <strong>U = 2</strong> in our case).

q[ <span style="color:#0000ff;">(1 + (U-1))</span> ] = p[1]<em>temp[0] + q[ <span style="color:#0000ff;">(1 + (U-1))</span> ]
q[ <span style="color:#0000ff;">(1 + U)</span> ] = p[1]</em>temp[1] + q[ <span style="color:#0000ff;">(1 + U)</span> ]
q[ <span style="color:#0000ff;">(1 + (U+1))</span> ] = p[1]*temp[2] + q[<span style="color:#0000ff;"> (1 + (U+1))</span> ]

You can verify that, substituting for U, you get the same as what you got earlier. That is,

q[<span style="color:#0000ff;">2</span>] = p[1]<em>temp[0] + q[<span style="color:#0000ff;">2</span>]
q[<span style="color:#0000ff;">3</span>] = p[1]</em>temp[1] + q[<span style="color:#0000ff;">3</span>]
q[<span style="color:#0000ff;">4</span>] = p[1]*temp[2] + q[<span style="color:#0000ff;">4]</span>

Good. But what about edge elements. Try doing the same for <strong>p[3]</strong> with <strong>U=2</strong>.

q[ <span style="color:#0000ff;">(3 + (U-1))</span> ] = p[3]<em>temp[0] + q[ <span style="color:#0000ff;">(3 + (U-1))</span> ]
q[ <span style="color:#0000ff;">(3 + U)</span> ] = p[3]</em>temp[1] + q[ <span style="color:#0000ff;">(3 + U)</span> ]
q[ <span style="color:#0000ff;">(3 + (U+1))</span> ] = p[3]*temp[2] + q[ <span style="color:#0000ff;">(3 + (U+1))</span> ]

becomes

q[ <span style="color:#0000ff;">4</span> ] = p[3]<em>temp[0] + q[<span style="color:#0000ff;"> 4</span> ]
q[ <span style="color:#0000ff;">5</span> ] = p[3]</em>temp[1] + q[ <span style="color:#0000ff;">5</span> ]
q[ <span style="color:#0000ff;">6</span> ] = p[3]*temp[2] + q[ <span style="color:#0000ff;">6</span> ]

Oh ho. You can see, this calculation simply overflows beyond max index of q. We said, our 1D is cyclic. Thus we should be getting,

q[<span style="color:#0000ff;">4</span>] = p[3]<em>temp[0] + q[<span style="color:#0000ff;">4</span>]
q[<span style="color:#0000ff;">0</span>] = p[3]</em>temp[1] + q[<span style="color:#0000ff;">0</span>]
q[<span style="color:#0000ff;">1</span>] = p[3]*temp[2] + q[<span style="color:#0000ff;">1</span>]

<strong>So after q[4], we should get q[0].</strong> Modulo operator comes to the rescue here.

Note that, <strong>5 % 5 = 0, 6 % 5 = 1</strong>, the exact numbers we need. Mod does not affect when numerator is smaller. for eg, <strong>4 % 5 is still 4.</strong> Perfect! Substituting that, and taking off the index calculations for q for brevity, we get the logic as below.

undershoot_index = <span style="color:#0000ff;">(i + (U-1)) % len(p)</span>
exact_index = <span style="color:#0000ff;">(i + (U)) % len(p)</span>
overshoot_index = <span style="color:#0000ff;">(i + (U+1)) % len(p)</span>

q[ <span style="color:#0000ff;">undershoot_index</span> ] = p[i]<em>temp[0] + q[ <span style="color:#0000ff;">undershoot_index</span> ]
q[ <span style="color:#0000ff;">exact_index</span> ] = p[i]</em>temp[1] + q[ <span style="color:#0000ff;">exact_index</span> ]
q[ <span style="color:#0000ff;">overshoot_index</span> ] = p[i]*temp[2] + q[ <span style="color:#0000ff;">overshoot_index</span> ]

Let us now try this generalized approach. This time, we use a for loop, and inside that, above logic.

```python
 p=[0, 0.5, 0, 0.5, 0]
 pExact = 0.8
 pOvershoot = 0.1
 pUndershoot = 0.1
 def move(p, U):
 """
 p = given probability distribution list
 U = amount of steps robot has moved to right (assumed as right for now)
 """
 ### INEXACT MOTION
 q = [ 0 for _ in p ]
 temp = [pUndershoot, pExact, pOvershoot]

for i in range(len(p)):
 undershoot_index = (i + (U-1)) % len(p)
 exact_index = (i + (U)) % len(p)
 overshoot_index = (i + (U+1)) % len(p)
 q[ undershoot_index ] = p[i]<em>temp[0] + q[ undershoot_index ]
 q[ exact_index ] = p[i]</em>temp[1] + q[ exact_index ]
 q[ overshoot_index ] = p[i]*temp[2] + q[ overshoot_index ]

return q

print("Prior probability: " + str(p))
 print("Posterior probability: " + str(move(p,2)))
```

<strong>Output:</strong>
<img class="alignnone size-full wp-image-191" src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_13h01_45.png" alt="2018-03-22_13h01_45" width="413" height="57" />

It works :)

You could try with different values of p, U and see that the program works.

<strong>Interesting observation:</strong> What if the robot keeps on moving 2 steps at a time. Intuitively over time, the uncertainty approaches maximum: robot is equally likely to be present in any cell. In other words, each cell would have (1/5) = 0.2 probability in our case of 1 D space of 5 cells.

<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_13h30_04.png"/>
<figcaption align='center'>Figure 10</figcaption>
</figure>
</div>

Let us check the code, calling the move for, say 1000 times.

```python
p=[0, 0.5, 0, 0.5, 0]
print("Prior probability: " + str(p))
for i in range(1000):
    p = move(p,2)
print("Posterior probability: " + str(move(p,2)))
```

Output:
<img class="alignnone size-full wp-image-193" src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_13h27_42.png" alt="2018-03-22_13h27_42" width="874" height="58" />

We got 0.2 for all cells :)

So when you just move every time, without 'sensing' any new information, uncertainty increases.