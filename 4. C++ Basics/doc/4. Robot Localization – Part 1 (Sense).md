# Part 1

This article explains one of the 2 main concepts discussed in Udacity's Self Driving Car -&gt; Bayesian Thinking -&gt; Lesson 11:Â  Robot Localization.

It would be highly helpful if you could glance upon my earlier articles, at least <a href="https://parthibanrajendran.wordpress.com/2018/03/17/0-bayes-theorem-intuitive-foundation/" target="_blank" rel="noopener">this one</a> before proceeding.

<strong>Situation</strong>: We have a 1 D world space with 5 different grid cells. They are colored red or green as shown below. Robot could be anywhere across this space, and all cells have equally likely chance. Thus we assume uniform probability distribution (1/5) = 0.2 for each cell. Also remember, the 1 D is cyclic, so if robot is in Cell 5, its next step is Cell 1. This is to keep our exercise simple.


<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-21_19h04_20.png"/>
<figcaption align='center'>Figure 1</figcaption>
</figure>
</div>

If X<sub>i</sub> represents the cell location, then,

$$
\displaystyle \begin{array}{l}p({{X}_{1}})=0.2\\p({{X}_{2}})=0.2\\p({{X}_{3}})=0.2\\p({{X}_{4}})=0.2\\p({{X}_{5}})=0.2\end{array}
$$

We also assume that, robot's sensor has accuracy as below.

<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-21_19h05_29.png"/>
<figcaption align='center'>Figure 2</figcaption>
</figure>
</div>


<strong>Problem</strong>: Robot senses that the cell it is standing on, is "<strong>Red</strong>". What is the probability of robot location in each cell X<sub>i</sub>?

Intuitively, then the probability of red cells should increase (as Robot mostly should be there), and green cells should decrease (as robot is not ideal and possibility of errors). Let us check mathematically. Using our earlier technique <a href="https://parthibanrajendran.wordpress.com/2018/03/17/0-bayes-theorem-intuitive-foundation/" target="_blank" rel="noopener">here</a>, we could apply the evidence on prior situation as below.


<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-21_19h26_03.png"/>
<figcaption align='center'>Figure 3</figcaption>
</figure>
</div>

For 1st cell which is green, there is 20% chance that sensor read it as "red". Thus probability of location being green <strong>and</strong> sensor reading as "red" would be <strong>20% out of 20%</strong> chance the green cell has. In other words, (0.2)(0.2) = 0.04. Similarly, we could calculate for all cells, and put in mathematical notation as below.

$\displaystyle \begin{array}{l}p({{X}_{1}}\cap Red)=\left( {0.2} \right)\left( {0.2} \right)=0.04\\p({{X}_{2}}\cap Red)=\left( {0.2} \right)(0.6)=0.12\\p({{X}_{3}}\cap Red)=\left( {0.2} \right)(0.6)=0.12\\p({{X}_{4}}\cap Red)=\left( {0.2} \right)\left( {0.2} \right)=0.04\\p({{X}_{5}}\cap Red)=\left( {0.2} \right)\left( {0.2} \right)=0.04\end{array}$

Summing up,

$$
\displaystyle \sum\limits_{{i=1}}^{n}{{p({{X}_{i}}\cap Red)}}=0.04+0.12+0.12+0.04+0.04=0.36  \tag{1}
$$

We have thus calculated joint probability. Coming to posterior which is also the problem to be solved: Given the sensor reading is "red", what is the probability for each cell? We could readily use Bayes' theorem

$$
\displaystyle \begin{array}{l}p\left( {{{X}_{i}}|\text{Red}} \right)\text{ }=\frac{{p({{X}_{i}}\cap \text{Red})}}{{p(\text{Red})}}\\\\\text{ }=\frac{{p({{X}_{i}}\cap \text{Red})}}{{\sum\limits_{{k=1}}^{n}{{p({{X}_{k}}\cap \text{Red}}})}}\end{array} \tag{2}
$$

Applying for each cell,

$ \displaystyle \begin{array}{l}p({{X}_{1}}|~\text{Red})=\left( {\frac{{0.04}}{{.36}}} \right)=0.111\\\\p({{X}_{2}}|~\text{Red})=\left( {\frac{{0.12}}{{.36}}} \right)=0.333\\\\p({{X}_{3}}|~\text{Red})=\left( {\frac{{0.12}}{{.36}}} \right)=0.333\\\\p({{X}_{4}}|~\text{Red})=\left( {\frac{{0.04}}{{.36}}} \right)=0.111\\\\p({{X}_{5}}|~\text{Red})=\left( {\frac{{0.04}}{{.36}}} \right)=0.111\end{array}$

Note that, posterior probability adds up to 1 again like prior.

$ \displaystyle \sum\limits_{{i=1}}^{n}{{p\left( {{{X}_{i}}|\text{Red}} \right)}}=1$

Also, again using Baye's theorem for RHS for (2), we could re write it as below.

$$
\displaystyle \begin{array}{l}p\left( {{{X}_{i}}|\text{Red}} \right)=\frac{{p({{X}_{i}}\cap \text{Red})}}{{\sum\limits_{{k=1}}^{n}{{p({{X}_{k}}\cap \text{Red)}}}}}\\\\\text{ }=\frac{{p(\text{Red }\!\!|\!\!\text{ }{{X}_{i}})p({{X}_{i}})}}{{\sum\limits_{{k=1}}^{n}{{p(\text{Red }\!\!|\!\!\text{ }{{X}_{k}})p({{X}_{k}})}}}}\end{array}
$$

Thus finally our posterior probability distribution in our visual summary:

<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-21_20h31_19.png"/>
<figcaption align='center'>Figure 4</figcaption>
</figure>
</div>


Note, that as intuitively imagined earlier, the probability for red cells have increased from 0.2 to 0.33 while the green cells has their probability decreased from 0.2 to 0.111. Now robot is more certain of its location :)

So remember, each time, robot senses' its certainty increases. Its more sure of its location.

<strong>Food for thought:</strong> What happens to the probability distribution if we ask robot to keep sensing for many times. Intuitively, then the red cells each should achieve 0.5 probability eventually and rest going to 0. Let us check this out pro grammatically after implementing sense function.

<strong>Python Implementation:</strong>

Suppose,

<span style="color:#0000ff;"><em>p</em></span> = list of prior probabilities of all cells
<span style="color:#0000ff;"><em>world</em></span> = list of color of each cell in same order (kinda map given to robot)
<span style="color:#0000ff;"><em>Z</em></span> = robot's measurement (in our test case, 'red')
<span style="color:#0000ff;"><em>pHit</em></span> = probability of robot correctly sensing red when its actually red
<span style="color:#0000ff;"><em>pMiss</em></span> = probability of robot wrongly sensing red, when its actually green

then,

<ol>
    <li>for each element in p, we could simply calculate the joint probability first by multiplying with pHit, if cell is red, or pMiss if cell is green</li>
    <li>calculate the p(Red) by summing up the joint probabilities</li>
    <li>calculate prior probability by diving each element again with the sum</li>
</ol>

Thus the implementation is as follows <em>(slightly different than Udacity's as we derived directly from logic without referring it)</em>.

```python
p=[0.2, 0.2, 0.2, 0.2, 0.2]
world=['green', 'red', 'red', 'green', 'green']
pHit = 0.6
pMiss = 0.2
def sense(p, Z):
    """
    p = initial probability distribution list
    Z = each measurement
    q = new normalized distribution list
    """
    q = []
    for each_index in range(len(p)):
        if (world[each_index] == Z):
            q.append(p[each_index]<em>pHit)
        else:
            q.append(p[each_index]</em>pMiss)
    total = sum(q)
    q = [ x/total for x in q]
    return q

print("Prior probability: " + str(p))
print("Posterior probability: " + str(sense(p,'red')))
```

Output:


<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-21_20h48_08.png"/>
<figcaption align='center'>Figure 5</figcaption>
</figure>
</div>


Coming back to food for thought, we could simulate asking robot to sense so many times by calling sense repeatedly with measurement as 'red'. Result is as expected.

```python
p=[0.2, 0.2, 0.2, 0.2, 0.2]
print("Prior probability: " + str(p))
for i in range(1000):
    p = sense(p, 'red')
print("Posterior probability: " + str(sense(p,'red')))
```

Output:

<div style="display: flex; justify-content: center;">
<figure>
<img src="http://www.rparthiban.com/articles/wp-content/uploads/2018/03/2018-03-22_15h05_27.png"/>
<figcaption align='center'>Figure 6</figcaption>
</figure>
</div>

So when you keep sensing new information, your certainty increases.