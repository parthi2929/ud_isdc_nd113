Soon after the Udacity author finishes Bayes, he extends it to Naive Bayes, and since our approach is different, now we need to understand the same via our lens once again.

<br>

<strong>Problem</strong>: We need to find out who probably sent the mail among Chris and Sara, provided a pattern observed in the mail. For brevity, we consider only 3 words, and assume the following.
<ol>
 	<li>Chris and Sara sending email are <em>mutually exclusive</em>. Its either Chris or Sara, never both and they both have equal chance</li>
 	<li>The 3 words (Love, Deal, Life) from them have following probabilities. Note these are <em>not mutually exclusive but independent.</em></li>
</ol>
<strong>Chris</strong>: Love (0.1), Deal (0.8), Life (0.1)  

<strong>Sara</strong>:  Love (0.5), Deal (0.2), Life (0.3)  

<br>

What is the probability that mail came from Chris, given the words "Life and Deal"? Similarly find for Sara as well.

Our tree diagram would be as follows initially.

<img class=" size-full wp-image-51 aligncenter" src="../../images/2018-03-09_20h39_44.png" alt="2018-03-09_20h39_44" width="402" height="440" />

Earlier I said, the probabilities of branches from a node add up to 1. Sorry, it was not always the case. For events that are mutually exclusive, this could be true but which are independent of each other, it need not be. This distinction would help us solve this puzzle, because, in above case, blue nodes (Chris/Sara) are mutually exclusive while orange (words), are independent of each other. I also would like to use an alternate notation.  

<br>

p(A and B) can also be represented and for better understanding as $ p(\text {A} \cap \text{B})$. Similarly p(A or B), which means probability of A or B or both, can be represented as $ p(\text {A} \cup \text{B})$  

<br>

As per basic probability summation rule for two events A and B,
<p style="text-align: center;">$ p(\text {A} \cup \text{B}) = p(\text{A}) +
p(\text{B}) - p(\text {A} \cap \text{B})$<span style="float: right;">(1)</span></p>  

<br>

Now for <em>mutually exclusive</em> events, they both can't happen together. Thus
<p style="text-align: center;">$ p(\text {A} \cap \text{B}) = 0$<span style="float: right;">(2)</span></p>  

<br>


For<em> independent events,</em> since they do not influence each other,  

<br>

<p style="text-align: center;">$ p(\text {A} \cap \text{B}) \neq 0$<span style="float: right;">(3)</span></p>  

Rather, from earlier Bayes theorem,  

<br>

<p style="text-align: center;">$ p(\text {A} \cap \text{B}) = p(\text{A})
p(\text {B} \mid \text {A}) $<span style="float: right;">(4)</span></p>  

<br>

Since A and B are independent,  

<br>

<p style="text-align: center;">$ p(\text {B} \mid \text {A}) = p(\text{B}) $<span style="float: right;">(5)</span></p>  

<br>

Thus,  
<p style="text-align: center;">$ p(\text {A} \cap \text{B}) = p(\text{A})
p(\text {B}) $<span style="float: right;">(6)</span></p>  

<br>

In terms of our problem, we could thus say from (1) and (2), as Chris and Sara are mutually exclusive,  

<br>

<p style="text-align: center;">$ p(\text {C} \cap \text{S}) = 0\\
p(\text {C} \cup \text{S}) = p(\text{C}) + p(\text{S}) $<span style="float: right;">(7)</span></p>  

<br>

Similarly, since words are independent of each other, for <em>"Life and Deal"</em>, we could say, from (1) and (6),  

<br>

<p style="text-align: center;">$ p(\text {Li} \cap \text{De}) = p(\text{Li})
p(\text{De}) \\
p(\text {Li} \cup \text{De}) = p(\text{Li}) + p(\text{De}) - p(\text {Li} \cap \text{De})
\\
p(\text {Li} \cup \text{De}) = p(\text{Li}) + p(\text{De}) - p(\text{Li})p(\text{De}) $<span style="float: right;">(8)</span></p>  

<br>

Applying p(Li ∩ De) = p(Li)p(De) in our tree we could condense it further as below.

<img class="alignnone size-full wp-image-68" src="../../images/2018-03-09_22h27_33.png" alt="2018-03-09_22h27_33" width="441" height="429" />  

<br>

Now our problem becomes simpler to tackle. We just need to find <strong>p(C | Li ∩ De)</strong> and <strong>p(S | Li ∩ De)</strong>  

<br>

From Figure 2, we can deduce p( C ∩ (Li ∩ De)) as below  

<img class=" size-full wp-image-69 aligncenter" src="../../images/2018-03-09_22h35_10.png" alt="2018-03-09_22h35_10" width="428" height="414" />  

<br>

<p style="text-align: center;">$ p(\text {C} \cap \text {Li} \cap \text{De}) =
(0.5)(0.08) = 0.04$<span style="float: right;">(9)</span></p>  

<br>

Also for p( S ∩ (Li ∩ De))..  

<img class=" size-full wp-image-71 aligncenter" src="../../images/2018-03-09_22h41_34.png" alt="2018-03-09_22h41_34" width="420" height="415" />  

<br>

<p style="text-align: center;">$ p(\text {S} \cap \text {Li} \cap \text{De}) =
(0.5)(0.06) = 0.03$<span style="float: right;">(10)</span></p>  

<br>

Further, p(Li ∩ De)..  

<img class=" size-full wp-image-72 aligncenter" src="../../images/2018-03-09_22h41_18.png" alt="2018-03-09_22h41_18" width="425" height="417" />  

<br>

<p style="text-align: center;">$ p(\text {Li} \cap \text{De}) =
p(\text {C} \cap \text {Li} \cap \text{De}) + p(\text {S} \cap \text {Li} \cap \text{De}) = (0.5)(0.08) + (0.5)(0.06) = 0.07$

<br>

<span style="float: right;">(11)</span></p>
Applying (9), (10), (11), via Bayes theorem, we get the required probabilities as below.  

<br>

<p style="text-align: center;">$ p(\text{C} \mid \text{Li} \cap \text{De})=\frac{p(\text {C} \cap \text {Li} \cap \text{De})}{p(\text {Li} \cap \text{De})} = \frac{0.04}{0.07} = 0.571$<span style="float: right;">(12)</span></p>  

<br>

<p style="text-align: center;">$ p(\text{S} \mid \text{Li} \cap \text{De})=\frac{p(\text {S} \cap \text {Li} \cap \text{De})}{p(\text {Li} \cap \text{De})} = \frac{0.03}{0.07} = 0.428$<span style="float: right;">(13)</span></p>  

<br>


Thus we have arrived at the answers via our lens. This also conforms with what is arrived in Udacity. :)

<img class="alignnone size-full wp-image-75" src="../../images/2018-03-09_22h58_42.png" alt="2018-03-09_22h58_42" width="807" height="415" />  

<br>

Note that the answer shown in Udacity are shown in shorter steps, but it is important to intuitively understand how do we do them that is why I showed step by step in detail. This way, further lectures in Bayesian domain would be now easier to comprehend.  

<br>

To justify the title (though focus here is to unravel the Udacity's lecture), Naive in Naive Bayes is that we simply assumed the words outcomes are independent of each other. This is why we just have to multiply their probabilities to arrive at a conclusion. If we are to assume they are dependent, things get complicated even with just 2 or 3 words. We are learning this to deploy on 1000s of words as part of machine learning, so we accept this disadvantage as long as its reasonable to assume the words are independent in given problem.  

<br>

In above problem, the labels are Chris and Sara (which we had to find out who likely sent mail given the words), and feature lists are the words.